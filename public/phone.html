<!-- This is the camera app -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Phone Camera Stream</title>
  <script src="face-api.min.js"></script>
  <style>
    body {
      margin: 0;
      font-family: 'Segoe UI', sans-serif;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      background-color: #121212;
      color: white;
      height: 100vh;
      overflow: hidden;
      transition: background-color 0.3s, color 0.3s;
    }

    video {
      width: 90%;
      max-width: 600px;
      border-radius: 12px;
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
      background: black;
    }

    .controls {
      margin-top: 20px;
      display: flex;
      gap: 10px;
    }

    button {
      padding: 12px 20px;
      font-size: 16px;
      border: none;
      border-radius: 8px;
      cursor: pointer;
      background: rgba(255, 255, 255, 0.1);
      color: inherit;
      transition: background 0.3s;
    }

    button:hover {
      background: rgba(255, 255, 255, 0.2);
    }

    .status {
      position: absolute;
      top: 20px;
      left: 20px;
      background: rgba(0, 0, 0, 0.7);
      padding: 10px;
      border-radius: 5px;
      font-size: 12px;
    height: calc(4em + 4px); /* 4 regels hoog, 2px extra boven en onder */
    line-height: 1em;
    margin-top: 2px;
    margin-bottom: 2px;
    display: flex;
    align-items: center;
    }

    .face-count {
      position: absolute;
      top: 20px;
      right: 20px;
      background: rgba(0, 100, 0, 0.8);
      padding: 10px 20px;
      border-radius: 5px;
      font-size: 16px;
      font-weight: bold;
    }

    #debugDiv {
      position: absolute;
      bottom: 0;
      left: 0;
      right: 0;
      background: rgba(0, 0, 0, 0.8);
      color: white;
      padding: 10px;
      font-size: 10px;
      overflow-y: auto;
      max-height: 100px;
      font-family: monospace;
    }
  </style>
</head>

<body>
  <div class="status" id="status">Initializing...</div>
  <div class="face-count" id="faceCount" style="display: none;">üë§ 0</div>

  <video autoplay playsinline muted></video>
  <div class="controls">
    <button id="startStop">‚ñ∂Ô∏è Start Detection</button>
    <button id="switchCamera">üì± Front</button>
  </div>

  <div id="debugDiv"></div>

  <script>
    // Debug console override
    const originalConsoleLog = console.log;
    console.log = function () {
      const debugDiv = document.getElementById("debugDiv");
      if (debugDiv) {
        const message = Array.prototype.slice.call(arguments).join(' ');
        debugDiv.innerHTML += message + '<br>';
        debugDiv.scrollTop = debugDiv.scrollHeight;
      }
      originalConsoleLog.apply(console, arguments);
    };

    const video = document.querySelector("video");
    const switchBtn = document.getElementById("switchCamera");
    const startStopBtn = document.getElementById("startStop");
    const statusDiv = document.getElementById("status");
    const faceCountDiv = document.getElementById("faceCount");

    let currentFacing = "user";
    let stream = null;
    let detecting = false;
    let detectionInterval = null;
    let brightnessInterval = null;
    let lastSentFaceHash = null; // Hash of last sent faces
    let sendBlocked = false;     // <-- BLOCKER: wait until count is 0 before sending again

    // Configurable image quality (0.1 = highly compressed, 1.0 = original)
    const IMAGE_QUALITY = 0.6;

    // Create hash from face positions for change detection
    function createFaceHash(detections) {
      if (!detections || detections.length === 0) return 'no-faces';

      return detections
        .map(d => `${Math.round(d.box.x/10)}-${Math.round(d.box.y/10)}-${Math.round(d.box.width/10)}-${Math.round(d.box.height/10)}`)
        .sort()
        .join('|');
    }

    // Draw face rectangles on canvas
    function drawFaceRectangles(canvas, ctx, detections) {
      ctx.strokeStyle = '#ff0000';
      ctx.lineWidth = 3;

      detections.forEach((detection, index) => {
        const { x, y, width, height } = detection.box;
        ctx.strokeRect(x, y, width, height);

        // Draw face number
        ctx.fillStyle = '#ff0000';
        ctx.font = '16px Arial';
        ctx.fillText(`${index + 1}`, x + 5, y + 20);
      });
    }

    function updateStatus(message) {
      if (statusDiv) {
        statusDiv.textContent = message;
      }
      console.log("Status:", message);
    }

    function updateFaceCount(count) {
      faceCountDiv.textContent = `üë§ ${count}`;
      faceCountDiv.style.display = count > 0 ? 'block' : 'none';
    }

    // Adjust light/dark theme
    function applyTheme(avgBrightness) {
      if (avgBrightness > 100) {
        document.body.style.backgroundColor = '#f0f0f0';
        document.body.style.color = '#121212';
      } else {
        document.body.style.backgroundColor = '#121212';
        document.body.style.color = 'white';
      }
    }

    // Start camera
    async function startCamera(facingMode = "user") {
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
      }

      try {
        updateStatus("Requesting camera access...");
        stream = await navigator.mediaDevices.getUserMedia({
          video: {
            facingMode,
            width: { ideal: 640 },
            height: { ideal: 480 }
          }
        });

        video.srcObject = stream;
        updateStatus("Camera started");

        // Start brightness monitoring
        if (brightnessInterval) clearInterval(brightnessInterval);
        const canvas = document.createElement("canvas");
        const ctx = canvas.getContext("2d", { willReadFrequently: true });

        brightnessInterval = setInterval(() => {
          if (video.videoWidth > 0) {
            canvas.width = Math.max(1, Math.floor(video.videoWidth / 4));
            canvas.height = Math.max(1, Math.floor(video.videoHeight / 4));
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

            const frame = ctx.getImageData(0, 0, canvas.width, canvas.height);
            let total = 0;
            for (let i = 0; i < frame.data.length; i += 4) {
              total += (frame.data[i] + frame.data[i + 1] + frame.data[i + 2]) / 3;
            }
            applyTheme(total / (canvas.width * canvas.height));
          }
        }, 500);

      } catch (err) {
        console.error("Camera access denied:", err);
        updateStatus("Camera access failed: " + (err && err.message ? err.message : err));
      }
    }

    // Wait for video to be ready
    async function waitForVideo() {
      return new Promise((resolve, reject) => {
        let attempts = 0;
        const maxAttempts = 50;

        const checkVideo = () => {
          attempts++;
          console.log(`Video check ${attempts}: readyState=${video.readyState}, size=${video.videoWidth}x${video.videoHeight}`);

          if (video.readyState >= 2 && video.videoWidth > 0 && video.videoHeight > 0) {
            console.log("Video is ready!");
            resolve();
          } else if (attempts >= maxAttempts) {
            reject(new Error("Video timeout"));
          } else {
            setTimeout(checkVideo, 100);
          }
        };

        checkVideo();
      });
    }

    // Start face detection
    async function startDetection() {
      if (detecting) return;

      try {
        updateStatus("Loading face detection models...");
        await faceapi.nets.tinyFaceDetector.loadFromUri('models');

        updateStatus("Waiting for video to be ready...");
        await waitForVideo();

        updateStatus("Starting face detection...");
        detecting = true;

        detectionInterval = setInterval(async () => {
          if (!detecting) return;

          try {
            const detections = await faceapi.detectAllFaces(
              video,
              new faceapi.TinyFaceDetectorOptions({ inputSize: 416 })
            );

            const count = detections.length;
            updateFaceCount(count);
            console.log(`Detected ${count} faces`);

            // --- send only when count > 0 and not blocked
            const currentFaceHash = createFaceHash(detections);

            if (count > 0) {
              if (!sendBlocked) {
                console.log('Face(s) detected and not blocked -> sending to backend...');
                sendBlocked = true;             // block until count returns to 0
                try {
                  await sendDetectionToBackend(count, detections);
                  // lastSentFaceHash = currentFaceHash; // optional, but not necessary
                } catch (err) {
                  console.error('Error in sendDetectionToBackend:', err);
                }
              } else {
                console.log('Faces detected but send is blocked until count returns to 0.');
              }
            } else {
              // count === 0 -> reset lock so next detection can send again
              if (sendBlocked) {
                console.log('No faces now -> unblocking future sends.');
              }
              sendBlocked = false;
              lastSentFaceHash = null;
            }

            updateStatus(`Detecting... (${count} faces)`);

          } catch (error) {
            console.error("Detection error:", error);
          }
        }, 1000);

      } catch (error) {
        console.error("Detection setup error:", error);
        updateStatus("Detection failed: " + (error && error.message ? error.message : error));
      }
    }

    // Send detection data to backend
    async function sendDetectionToBackend(count, detections) {
      try {
        console.log("Sending detection to backend...");

        // Create snapshot with face rectangles
        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth || 640;
        canvas.height = video.videoHeight || 480;
        const ctx = canvas.getContext('2d');

        // Draw video frame
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        // Draw face rectangles
        drawFaceRectangles(canvas, ctx, detections);

        // Get image data with configurable quality
        const imageData = canvas.toDataURL('image/jpeg', IMAGE_QUALITY);

        // Prepare detailed detection data
        const detectionData = detections.map((detection, index) => {
          const result = {
            faceNumber: index + 1,
            box: {
              x: Math.round(detection.box.x),
              y: Math.round(detection.box.y),
              width: Math.round(detection.box.width),
              height: Math.round(detection.box.height)
            },
            score: Math.round((detection.score || 0) * 1000) / 1000,
            center: {
              x: Math.round(detection.box.x + detection.box.width / 2),
              y: Math.round(detection.box.y + detection.box.height / 2)
            }
          };

          if (detection.landmarks && detection.landmarks.positions) {
            result.landmarks = detection.landmarks.positions.map(point => ({
              x: Math.round(point.x),
              y: Math.round(point.y)
            }));
          }

          if (detection.age !== undefined) result.age = Math.round(detection.age);
          if (detection.gender) result.gender = detection.gender;
          if (detection.genderProbability) result.genderProbability = Math.round(detection.genderProbability * 100) / 100;

          if (detection.expressions) {
            result.expressions = {};
            Object.keys(detection.expressions).forEach(expr => {
              result.expressions[expr] = Math.round(detection.expressions[expr] * 100) / 100;
            });
          }

          return result;
        });

        const response = await fetch("/api", {
          method: "POST",
          headers: {
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            count,
            image: imageData,
            timestamp: new Date().toISOString(),
            detections: detectionData,
            imageQuality: IMAGE_QUALITY,
            videoResolution: {
              width: video.videoWidth,
              height: video.videoHeight
            },
            facingMode: currentFacing
          })
        });

        if (response.ok) {
          console.log("Detection sent successfully");
        } else {
          console.error("Failed to send detection:", response.status);
        }

      } catch (error) {
        console.error("Error sending detection:", error);
      }
    }

    // Stop detection
    function stopDetection() {
      detecting = false;
      if (detectionInterval) {
        clearInterval(detectionInterval);
        detectionInterval = null;
      }
      updateFaceCount(0);
      updateStatus("Detection stopped");
      startStopBtn.textContent = "‚ñ∂Ô∏è Start Detection";
    }

    // Button events
    switchBtn.addEventListener("click", () => {
      currentFacing = currentFacing === "user" ? "environment" : "user";
      switchBtn.textContent = currentFacing === "user" ? "üì± Front" : "üì∑ Back";
      startCamera(currentFacing);
    });

    startStopBtn.addEventListener("click", () => {
      if (!detecting) {
        startStopBtn.textContent = "‚èπÔ∏è Stop";
        startDetection();
      } else {
        stopDetection();
      }
    });

    // Initialize
    startCamera(currentFacing);
  </script>

</body>
</html>
